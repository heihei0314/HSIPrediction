{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 1. Problem Statement\nThe project is aimed to predict closing price in Stock market. Prediction is always an interesting topic in ivestment. Investors believe that there is economic cycle in stock market. They would find hints in historical price data. Hence, many financial techique analysis were developed, such as Moving Average. In this project, it will make use of the concept of moving average and develop a LSTM RNN model to predict Closing price with Opening, Highest and Lowest price. ","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"jupyter":{"source_hidden":true,"outputs_hidden":true},"execution":{"iopub.status.busy":"2023-04-28T16:03:05.588535Z","iopub.execute_input":"2023-04-28T16:03:05.589299Z","iopub.status.idle":"2023-04-28T16:03:05.615101Z","shell.execute_reply.started":"2023-04-28T16:03:05.589241Z","shell.execute_reply":"2023-04-28T16:03:05.613655Z"},"collapsed":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Exploratory Data Analysis (EDA)\nThe dataset is reference from Kaggle Dataset: [Stock Exchange Data](https://www.kaggle.com/datasets/mattiuzc/stock-exchange-data) collaborated by Cody. This dataset contains historic data for a variety of stocks exchange market from Yahoo Finance. In this project, we are interested in the Hong Kong Stock Exchange (HSI) from 2011 to 2021. ","metadata":{}},{"cell_type":"code","source":"#grabing the dataset \ndf = pd.read_csv(\"/kaggle/input/stock-exchange-data/indexData.csv\")\n\n#select interested data\nHSI = df[df[\"Index\"] == 'HSI']\nHSI2011 = HSI[HSI[\"Date\"] >= '2011-01-01']\nHSI2011 = HSI2011.dropna()\nHSI2011.head()","metadata":{"execution":{"iopub.status.busy":"2023-04-28T16:03:05.617710Z","iopub.execute_input":"2023-04-28T16:03:05.619004Z","iopub.status.idle":"2023-04-28T16:03:05.956090Z","shell.execute_reply.started":"2023-04-28T16:03:05.618953Z","shell.execute_reply":"2023-04-28T16:03:05.954784Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Data Size\nThere are 2558 records (rows) and 8 features (columns) in total. The total memory usage of the data is about 179.9KB.\n\n#### Data dimension \nrefers to the number of attributes or variables that are being measured for each data point. It can be thought of as the number of columns or features in a dataset. For example, \nThe dataset includes information about the index name (HSI), Date, pricing at different stage, and the volume of exchange. The details of explaination are as below:\n>* Index: The name of the stock index. In this case, only HSI is stored. It refers to Hang Seng Index, the stock exchange index in Hong Kong.\n>* Date: The record date of the exchange. It set from 2011-01-01 to 2021-12-31.\n>* Open: The price at the begining of the exchange date.\n>* High: The highest price on the exchange date.\n>* Low: The lowest price on the exchange date.\n>* Close: The price at the end of the exchange date. It is also the target features in this project.\n>* Adj Close: Adjusted closing price refers to the price of the stock after paying off the dividends. In this case, it is always as same as the Close\n>* Volume: The volume of exchange on the exchange date.\n\n#### Data structure \nThere are 2 object features and 6 Float Features in the dataset. All features are non-null value as the rest dates of exchange are excluded.","metadata":{}},{"cell_type":"code","source":"#Data Infomation\nprint(HSI2011.info(verbose=True))","metadata":{"execution":{"iopub.status.busy":"2023-04-28T16:03:05.957733Z","iopub.execute_input":"2023-04-28T16:03:05.958415Z","iopub.status.idle":"2023-04-28T16:03:05.992010Z","shell.execute_reply.started":"2023-04-28T16:03:05.958376Z","shell.execute_reply":"2023-04-28T16:03:05.990697Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Descriptive Statistics\nThe four pricing index are close to each others and having the same trend. The HSI is fraustrated but has a slight upward trend. The highest closing price is 33154.12109, and lowest is 16250.26953. The average of HSI is 24328.96704774824. The volume of exchange are consistant, and have cycle. ","metadata":{}},{"cell_type":"code","source":"#descriptive data\nprint(\"max: \", max(HSI2011['Close']))\nprint(\"min: \", min(HSI2011['Close']))\nprint(\"mean: \", np.mean(HSI2011['Close']))\n\n#Visualize the dataset\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nHSI2011.loc[: ,\"Date\"] = pd.to_datetime(HSI2011.loc[: ,\"Date\"])\nplt.figure(figsize=(24,8))\nplt.title(\"Pricing HSI from 2011 to 2021\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Price\")\nplt.plot(HSI2011['Date'], HSI2011['Close'])\nplt.plot(HSI2011['Date'], HSI2011['Open'])\nplt.plot(HSI2011['Date'], HSI2011['High'])\nplt.plot(HSI2011['Date'], HSI2011['Low'])\nplt.legend([\"Close\",\"Open\",\"High\",\"Low\"])\ndtFmt = mdates.DateFormatter('%Y-%b') # define the formatting\nplt.gca().xaxis.set_major_formatter(dtFmt) \n# show every quater \nplt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=3))\nplt.xticks(rotation = 45)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-28T16:03:05.997420Z","iopub.execute_input":"2023-04-28T16:03:05.997844Z","iopub.status.idle":"2023-04-28T16:03:06.894876Z","shell.execute_reply.started":"2023-04-28T16:03:05.997791Z","shell.execute_reply":"2023-04-28T16:03:06.893868Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.title(\"Volume of HSI from 2011 to 2021\")\nplt.bar(HSI2011['Date'], HSI2011['Volume'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-28T16:03:06.895940Z","iopub.execute_input":"2023-04-28T16:03:06.896261Z","iopub.status.idle":"2023-04-28T16:03:11.680271Z","shell.execute_reply.started":"2023-04-28T16:03:06.896229Z","shell.execute_reply":"2023-04-28T16:03:11.679042Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Simple Moving Average is one of the common used technique for investors to predict the price. 10, 20, 50, 100, 200, 250 are the most popular time step used for the indicators.\n\nWe could see that MA20 and MA50 are most fit to the HSI without overfitting. We might consider it for the time step of our predict model. \n","metadata":{}},{"cell_type":"code","source":"#Simple Moving Average Calculation\ndef MA(dataset, t):\n    MA=np.empty(len(dataset), dtype=object) \n    for i in range(t-1,len(dataset)):\n        MA[i]=np.mean(dataset['Close'][i-t+1:i])\n        #print(i) \n    return MA \n\n# find the MA10, MA20, MA50, MA100, MA200, MA250\nHSI2011['MA10']=MA(HSI2011,10)\nHSI2011['MA20']=MA(HSI2011,20)\nHSI2011['MA50']=MA(HSI2011,50)\nHSI2011['MA100']=MA(HSI2011,100)\nHSI2011['MA200']=MA(HSI2011,200)\nHSI2011['MA250']=MA(HSI2011,250)\n\n#plot the MA\nplt.figure(figsize=(24,8))\nplt.title(\"MA for HSI from 2011 to 2021\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Price\")\nplt.plot(HSI2011['Date'], HSI2011['Close'])\nplt.plot(HSI2011['Date'], HSI2011['MA10'])\nplt.plot(HSI2011['Date'], HSI2011['MA20'])\nplt.plot(HSI2011['Date'], HSI2011['MA50'])\nplt.plot(HSI2011['Date'], HSI2011['MA100'])\nplt.plot(HSI2011['Date'], HSI2011['MA200'])\nplt.plot(HSI2011['Date'], HSI2011['MA250'])\nplt.legend([\"Close\",\"MA10\",\"MA20\",\"MA50\",\"MA100\",\"MA200\",\"MA250\"])\ndtFmt = mdates.DateFormatter('%Y-%b') # define the formatting\nplt.gca().xaxis.set_major_formatter(dtFmt) \n# show every quater \nplt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=3))\nplt.xticks(rotation = 45)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-28T16:03:11.681641Z","iopub.execute_input":"2023-04-28T16:03:11.681981Z","iopub.status.idle":"2023-04-28T16:03:14.506450Z","shell.execute_reply.started":"2023-04-28T16:03:11.681948Z","shell.execute_reply":"2023-04-28T16:03:14.500715Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Pre-proceed data\nFirst, The data will be splited into Training, Validation and Testing Data set. The Testing Data set will use the HSI since 2020. The rest of the data (2011-2019) will be used for training and validation. The data in 2011-2019, 20% will be used for validation. As a result, 348 testing data, and 2210 training and validation data.\n\nSecond, due to the charaatics of the data features, Adj Close, Index, Date will be drop out. They are not affecting the prediction and waste calucation resources. Then reshape the training dataset by desired time steps.\n\nLast, to converge faster and stablize the gradient descent step. It is going to normalize the data scale with normalization layer.","metadata":{}},{"cell_type":"code","source":"# grab data for training data\ntrain_data = HSI2011[HSI2011[\"Date\"] < '2020-01-01'].reset_index(drop=True)\nprint(train_data.head())\n\ndef dataPreproceed(data_set, timestep):\n    global x_train\n    global y_train\n    \n    #grab the interested pricing and volumn (dropout Adj Close, Index, Date and the MAs) \n    new_data_x = data_set[[\"Open\", \"High\", \"Low\", \"Volume\"]]\n    new_data_y = np.asarray(data_set['Close'])\n    \n    #reshape with desired input shape \n    for i in range(timestep, len(new_data_x)):\n        x_train.append(new_data_x[i-timestep:i])\n        y_train.append(new_data_y[i])\n        \n    x_train = np.array(x_train)\n    y_train = np.array(y_train)\n    print(x_train.shape)\n    print(y_train.shape)\n    return \n\n# pre-proceed the data\nx_train=[]\ny_train=[]\ntimestep = 20\ndataPreproceed(train_data, timestep)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-28T16:03:14.508285Z","iopub.execute_input":"2023-04-28T16:03:14.509037Z","iopub.status.idle":"2023-04-28T16:03:14.615261Z","shell.execute_reply.started":"2023-04-28T16:03:14.508995Z","shell.execute_reply":"2023-04-28T16:03:14.613798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Model Architecture\nAs mentioned, we assume that the stock price has correlation in historical data. It indicates that it is a time series problem. LSTM in RNN is suggested for this problem. This approach consider the time effect on the output. The suggested Architecture is as follow:\n> Input -> LSTM -> BatchNormalization -> Dropout -> Dense(4)* -> Dense(1) -> Output\n\n*The first Dense layer is set as 4 becuase of 4 input features.\n\n#### Return_sequences \nIt will be set as **FALSE** as the order sequence is important for stock and would not have effect when return back.\n\n#### Activation Function and Optimization Method\nAccording to Masud Rana (2019), Adam optimizator and tanh for LSTM layer preformed the best. Hence, this project will adopt in this setting as initiation. \n\n#### Time Step and the Units of LSTM\nAs discussed, the moving average performs well at **20** and **50**. Referring this setting. This project will try the **20** and **50** time step with the same units of LSTM. Some analysist would like to use both indicators at the same time. Hence, this project will try multiple LSTM layers with **50** and **20** units. The first layer will set return_sequences as **TRUE**.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.optimizers import Adam, SGD, RMSprop\n\nlearning_rate = 0.01\nvalidationSplit = 0.2 \nepochs = 100\nbatch = 64\n\n#build the first model with 20 time step approach\nmodel_20 = keras.Sequential()\nmodel_20.add(layers.LSTM(units = x_train.shape[1], return_sequences = False, input_shape = (x_train.shape[1],x_train.shape[2]), activation='tanh'))\nmodel_20.add(layers.BatchNormalization())\nmodel_20.add(layers.Dropout(0.5))\nmodel_20.add(layers.Dense(x_train.shape[2]))\nmodel_20.add(layers.Dense(1))\nmodel_20.build()\nmodel_20.summary()\n\nopt = Adam(learning_rate=learning_rate)\nmodel_20.compile(optimizer=opt, loss='mean_absolute_error')\nhist_20 = model_20.fit(x_train, y_train, batch_size = batch, validation_split = validationSplit, epochs = epochs)","metadata":{"execution":{"iopub.status.busy":"2023-04-28T16:03:14.617586Z","iopub.execute_input":"2023-04-28T16:03:14.618088Z","iopub.status.idle":"2023-04-28T16:04:11.911909Z","shell.execute_reply.started":"2023-04-28T16:03:14.618039Z","shell.execute_reply":"2023-04-28T16:04:11.910647Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pre-proceed the data\nx_train=[]\ny_train=[]\ntimestep = 50\ndataPreproceed(train_data, timestep)\n\n#build the second model with 50 time step approach\nmodel_50 = keras.Sequential()\nmodel_50.add(layers.LSTM(units = x_train.shape[1], return_sequences = False, input_shape = (x_train.shape[1],x_train.shape[2]), activation='tanh'))\nmodel_50.add(layers.BatchNormalization())\nmodel_50.add(layers.Dropout(0.5))\nmodel_50.add(layers.Dense(x_train.shape[2]))\nmodel_50.add(layers.Dense(1))\nmodel_50.build()\nmodel_50.summary()\n\nopt = Adam(learning_rate=learning_rate)\nmodel_50.compile(optimizer=opt, loss='mean_absolute_error')\nhist_50 = model_50.fit(x_train, y_train, batch_size = batch, validation_split = validationSplit, epochs = epochs)","metadata":{"execution":{"iopub.status.busy":"2023-04-28T16:04:11.914091Z","iopub.execute_input":"2023-04-28T16:04:11.914471Z","iopub.status.idle":"2023-04-28T16:05:36.666755Z","shell.execute_reply.started":"2023-04-28T16:04:11.914435Z","shell.execute_reply":"2023-04-28T16:05:36.665308Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pre-proceed the data\nx_train=[]\ny_train=[]\ntimestep = 50\ndataPreproceed(train_data, timestep)\n\n#build the third model with mixed approach\nmodel_mix = keras.Sequential()\nmodel_mix.add(layers.LSTM(units = x_train.shape[1], return_sequences = True, input_shape = (x_train.shape[1],x_train.shape[2]), activation='tanh'))\nmodel_mix.add(layers.BatchNormalization())\nmodel_mix.add(layers.Dropout(0.5))\n\nmodel_mix.add(layers.LSTM(units = 20, return_sequences = False, input_shape = (x_train.shape[1],x_train.shape[2]), activation='tanh'))\nmodel_mix.add(layers.BatchNormalization())\nmodel_mix.add(layers.Dropout(0.5))\nmodel_mix.add(layers.Dense(x_train.shape[2]))\nmodel_mix.add(layers.Dense(1))\nmodel_mix.build()\nmodel_mix.summary()\n\nopt = Adam(learning_rate=learning_rate)\nmodel_mix.compile(optimizer=opt, loss='mean_absolute_error')\nhist_mix = model_mix.fit(x_train, y_train, batch_size = batch, validation_split = validationSplit, epochs = epochs)","metadata":{"execution":{"iopub.status.busy":"2023-04-28T16:05:36.669935Z","iopub.execute_input":"2023-04-28T16:05:36.671189Z","iopub.status.idle":"2023-04-28T16:08:04.481577Z","shell.execute_reply.started":"2023-04-28T16:05:36.671123Z","shell.execute_reply":"2023-04-28T16:08:04.480530Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Result and Analysis\nAll model perform good learning. The model mixed and 20 time step performed better. However all of them show overfitting after 10 epoches. The model mixed is less serious. Hence, Model mixed would be chosen for testing prediction.","metadata":{}},{"cell_type":"code","source":"plt.plot(hist_20.history[\"loss\"])\n\nplt.plot(hist_50.history[\"loss\"])\n\nplt.plot(hist_mix.history[\"loss\"])\n\nplt.title(\"Model Evaluation\")\nplt.ylabel(\"mean_absolute_error\")\nplt.xlabel(\"Epoch\")\nplt.legend([\"20_Training Loss\", \"50_Training Loss\",\"mixed_Training Loss\"])\nplt.show()\n\nplt.plot(hist_20.history[\"loss\"])\nplt.plot(hist_20.history['val_loss'])\nplt.title(\"Model 20 time step Evaluation\")\nplt.ylabel(\"mean_absolute_error\")\nplt.xlabel(\"Epoch\")\nplt.legend([\"Training Loss\",\"Validation Loss\"])\nplt.show()\n\nplt.plot(hist_50.history[\"loss\"])\nplt.plot(hist_50.history['val_loss'])\nplt.title(\"Model 50 time step Evaluation\")\nplt.ylabel(\"mean_absolute_error\")\nplt.xlabel(\"Epoch\")\nplt.legend([\"Training Loss\",\"Validation Loss\"])\nplt.show()\n\nplt.plot(hist_mix.history[\"loss\"])\nplt.plot(hist_mix.history['val_loss'])\nplt.title(\"Model mixed Evaluation\")\nplt.ylabel(\"mean_absolute_error\")\nplt.xlabel(\"Epoch\")\nplt.legend([\"Training Loss\",\"Validation Loss\"])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-28T16:08:04.485903Z","iopub.execute_input":"2023-04-28T16:08:04.486336Z","iopub.status.idle":"2023-04-28T16:08:05.465755Z","shell.execute_reply.started":"2023-04-28T16:08:04.486280Z","shell.execute_reply":"2023-04-28T16:08:05.464478Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Prediction for Testing\nThe Testing Data set will use the HSI since 2020. The rest of the data (2011-2019) is used for testing data. There are 348 entries. The model mixed and 50 time steps would be used.","metadata":{}},{"cell_type":"code","source":"# grab data for testing data\ntest_data = HSI2011[HSI2011[\"Date\"] >= '2020-01-01'].reset_index(drop=True)\nprint(test_data.info())\nx_train=[]\ny_train=[]\ntimestep = 50\ndataPreproceed(test_data, timestep)\n\n#predict for testing\npredictions = model_mix.predict(x_train, verbose=1)\npredictions = np.transpose(predictions)\nprint(predictions)","metadata":{"execution":{"iopub.status.busy":"2023-04-28T16:28:31.328045Z","iopub.execute_input":"2023-04-28T16:28:31.328446Z","iopub.status.idle":"2023-04-28T16:28:31.591177Z","shell.execute_reply.started":"2023-04-28T16:28:31.328410Z","shell.execute_reply":"2023-04-28T16:28:31.589828Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Evaluate the testing result\nae = np.abs(predictions-y_train)\nprint('MAE:', np.mean(ae[0]))\nx = np.arange(1, len(ae[0])+1)\nplt.plot(x, ae[0])\nplt.title(\"Testing Evaluation\")\nplt.ylabel(\"absolute_error\")\nplt.xlabel(\"N\")\nplt.legend([\"Testing Loss\"])\nplt.show()\n\nplt.plot(y_train)\nplt.plot(predictions)\nplt.plot(ae[0])\nplt.title(\"Testing Evaluation\")\nplt.ylabel(\"Close\")\nplt.xlabel(\"N\")\nplt.legend([\"Actual\", \"Predict\", 'Loss'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-28T16:28:18.824211Z","iopub.execute_input":"2023-04-28T16:28:18.824666Z","iopub.status.idle":"2023-04-28T16:28:19.623363Z","shell.execute_reply.started":"2023-04-28T16:28:18.824612Z","shell.execute_reply":"2023-04-28T16:28:19.621667Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. Conclusion\nAs a result, the model perform well at the first few time step. After the 50 time step, it return constant. The MAE is 4580.389622516779. The model is overfitted and not accurate as expected. The architecure and parameters (units of Dropout and LSTM layer) an should be modified to improve. ","metadata":{}}]}